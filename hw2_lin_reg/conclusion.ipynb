{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы по работе\n",
    "\n",
    "В данной работе был реализован и протестирован ряд функций, используемых в алгоритме линейной регрессии - MSE, MAE (функции потерь), функции регуляризаций (чтобы тюнить параметры модели в зависимости от используемой функции потерь), а также проивзводные этих функций для градиентного спуска (минимизации функций потерь).\n",
    "\n",
    "### Отличие MSE и MAE\n",
    "\n",
    "Среднеквадратичная ошибка (MSE - Mean Squared Error) представляет собой среднее значение квадратов ошибок между реальными и прогнозируемыми значениями. MSE учитывает как небольшие, так и большие ошибки и дает больший вес большим ошибкам из-за квадратичного компонента (т.е. более чувствительна к выбросам).\n",
    "\n",
    "Средняя абсолютная ошибка (MAE - Mean Absolute Error) представляет собой среднее значение абсолютных разностей между реальными и прогнозируемыми значениями. MAE менее чувствительна к большим ошибкам (выбросам), поскольку она не учитывает квадрат ошибок, и применяется, когда все ошибки имеют одинаковый вес.\n",
    "\n",
    "### Отличие L1 и L2\n",
    "\n",
    "L1 и L2 регуляризации используются в задаче линейной регрессии для предотвращения переобучения, улучшения обобщающей способности модели и устойчивости, а также для обработки мультиколлинеарности между признаками.\n",
    "\n",
    "L1-регуляризация (Lasso-регуляризация) добавляет абсолютное значение весовых коэффициентов к функции потерь. Она имеет свойство приводить некоторые веса к нулю, что может рассматриваться как автоматический отбор признаков. Это может быть полезным, когда у нас есть большое количество признаков, и мы хотим сделать модель более интерпретируемой и избавиться от незначимых признаков.\n",
    "\n",
    "L2-регуляризация (Ridge-регуляризация) добавляет квадрат весовых коэффициентов к функции потерь. Она уменьшает веса, однако не делает их равными нулю. L2-регуляризация работает хорошо, когда у нас есть мультиколлинеарность между признаками, то есть когда несколько признаков сильно коррелируют друг с другом. L2-регуляризация уменьшает веса коррелирующих признаков, делая модель более устойчивой к изменениям в данных.\n",
    "\n",
    "Обе регуляризации являются методами, которые позволяют балансировать между сложностью модели и ее способностью обобщать информацию на новых данных. В некоторых случаях также используется комбинация обеих регуляризаций, называемая Elastic Net.\n",
    "\n",
    "### Итог\n",
    "\n",
    "В рукописной реализации линейной регрессии MSE равна 48.4, а в реализации scikit-learn - MSE = 42.5. С учетом того, что мы заранее знаем об ошибке bias term (невозможности его зафитить), это неплохой результат по сравнению с реализацией \"из коробки\"."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
